{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c14eeb-debe-45fb-9e05-09b55832181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Constants\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\raw\\npp\"\n",
    "BASE_URL_TEMPLATE = \"https://npp.gov.in/public-reports/cea/daily/dgr/{date_dmy}/dgr{report_number}-{date_ymd}.xls\"\n",
    "\n",
    "# Finalized date range: Jan 1, 2022 to today\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime.today()\n",
    "\n",
    "# Most relevant report numbers: overview, breakdowns, capacity, fuel types\n",
    "report_numbers = ['1', '2', '16', '17']  # Add more if needed, e.g., '10A', '6' for reservoir\n",
    "\n",
    "def download_reports():\n",
    "    for date in (start_date + timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "        date_dmy = date.strftime(\"%d-%m-%Y\")\n",
    "        date_ymd = date.strftime(\"%Y-%m-%d\")\n",
    "        target_folder = os.path.join(SAVE_DIR, date_ymd)\n",
    "        os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "        for report_num in report_numbers:\n",
    "            filename = f\"dgr{report_num}-{date_ymd}.xls\"\n",
    "            url = BASE_URL_TEMPLATE.format(date_dmy=date_dmy, report_number=report_num, date_ymd=date_ymd)\n",
    "            filepath = os.path.join(target_folder, filename)\n",
    "\n",
    "            if os.path.exists(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200 and response.headers.get(\"Content-Type\", \"\").startswith(\"application/vnd.ms-excel\"):\n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"‚úÖ Downloaded: {filename}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Not available or not Excel: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error on {filename}: {e}\")\n",
    "\n",
    "# Run it\n",
    "download_reports()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886a083-e97c-45c1-b8b5-41619e5bf8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Save path\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\raw\\posoco\"\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime.today()\n",
    "\n",
    "# Generate financial year string\n",
    "def get_fy_range(date):\n",
    "    if date.month >= 4:\n",
    "        return f\"{date.year}-{date.year + 1}\"\n",
    "    else:\n",
    "        return f\"{date.year - 1}-{date.year}\"\n",
    "\n",
    "# Month folder like \"January+2022\"\n",
    "def format_month(date):\n",
    "    return date.strftime(\"%B\") + \"+\" + date.strftime(\"%Y\")\n",
    "\n",
    "# Filename like \"02.01.22_NLDC_PSP.pdf\"\n",
    "def format_filename(date):\n",
    "    return date.strftime(\"%d.%m.%y\") + \"_NLDC_PSP.pdf\"\n",
    "\n",
    "# Build the download URL using &dl=\n",
    "def build_download_url(date):\n",
    "    fy = get_fy_range(date)\n",
    "    month_folder = format_month(date)\n",
    "    filename = format_filename(date)\n",
    "    return f\"https://report.grid-india.in/index.php?p=Daily+Report%2FPSP+Report%2F{fy}%2F{month_folder}&dl={filename}\"\n",
    "\n",
    "# Main downloader\n",
    "def download_psp_reports():\n",
    "    for single_date in (start_date + timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "        folder_name = single_date.strftime(\"%Y-%m-%d\")\n",
    "        filename = format_filename(single_date)\n",
    "        file_url = build_download_url(single_date)\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"‚úîÔ∏è Already downloaded: {filename}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(file_url, timeout=10)\n",
    "            if response.status_code == 200 and \"pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"‚úÖ Downloaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error downloading {filename}: {e}\")\n",
    "\n",
    "# Run it\n",
    "download_psp_reports()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206774f-ea9a-4a67-a33b-6166f6acde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Where to save the VRE reports\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\raw\\posoco-vre\"\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2025, 1, 1)\n",
    "end_date = datetime.today()\n",
    "\n",
    "def get_fy_range(date):\n",
    "    if date.month >= 4:\n",
    "        return f\"{date.year}-{date.year + 1}\"\n",
    "    else:\n",
    "        return f\"{date.year - 1}-{date.year}\"\n",
    "\n",
    "def format_month(date):\n",
    "    return date.strftime(\"%B\") + \"+\" + date.strftime(\"%Y\")\n",
    "\n",
    "def format_filename(date):\n",
    "    return date.strftime(\"%d.%m.%Y\") + \"_NLDC_REMC_REPORT.pdf\"\n",
    "\n",
    "def build_download_url(date):\n",
    "    fy = get_fy_range(date)\n",
    "    month_folder = format_month(date)\n",
    "    filename = format_filename(date)\n",
    "    return f\"https://report.grid-india.in/index.php?p=Daily+Report%2FVRE%2F{fy}%2F{month_folder}&dl={filename}\"\n",
    "\n",
    "def download_vre_reports():\n",
    "    for single_date in (start_date + timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "        folder_name = single_date.strftime(\"%Y-%m-%d\")\n",
    "        filename = format_filename(single_date)\n",
    "        file_url = build_download_url(single_date)\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, folder_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"‚úîÔ∏è Already downloaded: {filename}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(file_url, timeout=10)\n",
    "            if response.status_code == 200 and \"pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"‚úÖ Downloaded: {filename}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error downloading {filename}: {e}\")\n",
    "\n",
    "# Run it\n",
    "download_vre_reports()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd062c1-0a2b-404b-abde-21711998f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Save location\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\raw\\posoco-forecast-error\"\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2024, 12, 15)\n",
    "end_date = datetime.today()\n",
    "\n",
    "# Build FY format for POSOCO\n",
    "def get_fy_range(date):\n",
    "    return f\"{date.year}-{date.year + 1}\" if date.month >= 4 else f\"{date.year - 1}-{date.year}\"\n",
    "\n",
    "# Format filename like \"05-Feb-2024.pdf\"\n",
    "def format_filename(date):\n",
    "    return date.strftime(\"%d-%b-%Y\") + \".pdf\"\n",
    "\n",
    "# Build full URL\n",
    "def build_url(date):\n",
    "    fy = get_fy_range(date)\n",
    "    filename = format_filename(date)\n",
    "    return f\"https://report.grid-india.in/index.php?p=Forecast+Error+Report%2FDaily%2F{fy}&dl={filename}\"\n",
    "\n",
    "# Reliable download function\n",
    "def safe_download(url, path, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200 and \"pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "                with open(path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"‚úÖ Downloaded: {os.path.basename(path)}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {os.path.basename(path)}\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Retry {attempt + 1}/{retries} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    print(f\"‚ùå Failed after {retries} attempts: {os.path.basename(path)}\")\n",
    "    return False\n",
    "\n",
    "# Main loop\n",
    "def download_forecast_error_reports():\n",
    "    for single_date in (start_date + timedelta(n) for n in range((end_date - start_date).days + 1)):\n",
    "        folder = single_date.strftime(\"%Y-%m-%d\")\n",
    "        filename = format_filename(single_date)\n",
    "        file_url = build_url(single_date)\n",
    "\n",
    "        save_path = os.path.join(SAVE_DIR, folder)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"‚úîÔ∏è Already downloaded: {filename}\")\n",
    "            continue\n",
    "\n",
    "        safe_download(file_url, file_path)\n",
    "\n",
    "# Run it\n",
    "download_forecast_error_reports()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216d146-0e39-4049-adc9-625b26675435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\raw\\weather\"\n",
    "START_DATE = \"2022-01-01\"\n",
    "END_DATE = \"2025-05-22\"\n",
    "\n",
    "# List of representative cities including NE India\n",
    "LOCATIONS = [\n",
    "    {\"name\": \"Delhi\",      \"lat\": 28.6,   \"lon\": 77.2},\n",
    "    {\"name\": \"Bengaluru\",  \"lat\": 12.97,  \"lon\": 77.59},\n",
    "    {\"name\": \"Ahmedabad\",  \"lat\": 23.03,  \"lon\": 72.58},\n",
    "    {\"name\": \"Kolkata\",    \"lat\": 22.57,  \"lon\": 88.36},\n",
    "    {\"name\": \"Rewa\",       \"lat\": 24.53,  \"lon\": 81.30},\n",
    "    {\"name\": \"Guwahati\",   \"lat\": 26.1445,\"lon\": 91.7362}  # North-East zone\n",
    "]\n",
    "\n",
    "# Daily variables to fetch\n",
    "DAILY_PARAMS = [\n",
    "    \"temperature_2m_max\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"precipitation_sum\",\n",
    "    \"wind_speed_10m_max\",\n",
    "    \"cloudcover_mean\",\n",
    "    \"shortwave_radiation_sum\"\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION ----------\n",
    "def download_weather_data(location):\n",
    "    print(f\"\\nüì° Fetching weather data for {location['name']}...\")\n",
    "\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": location[\"lat\"],\n",
    "        \"longitude\": location[\"lon\"],\n",
    "        \"start_date\": START_DATE,\n",
    "        \"end_date\": END_DATE,\n",
    "        \"daily\": \",\".join(DAILY_PARAMS),\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed for {location['name']}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    data = response.json().get(\"daily\")\n",
    "    if not data:\n",
    "        print(f\"‚ö†Ô∏è No data for {location['name']}\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.insert(0, \"location\", location[\"name\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    save_path = os.path.join(SAVE_DIR, f\"{location['name']}.csv\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def run_all():\n",
    "    for loc in LOCATIONS:\n",
    "        download_weather_data(loc)\n",
    "\n",
    "run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8c393-7671-4228-9653-b74ed1c48991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "RAW_DIR = r\"E:\\energy-optimization-project\\data\\raw\\weather\"\n",
    "SAVE_DIR = r\"E:\\energy-optimization-project\\data\\processed\\weather\"\n",
    "OUTPUT_FILE = \"India_Averaged_Weather.csv\"\n",
    "\n",
    "# ---------- Load All Regional Files ----------\n",
    "weather_dfs = []\n",
    "for filename in os.listdir(RAW_DIR):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(RAW_DIR, filename))\n",
    "        df[\"location\"] = filename.replace(\".csv\", \"\")\n",
    "        weather_dfs.append(df)\n",
    "\n",
    "# ---------- Combine & Average ----------\n",
    "combined_df = pd.concat(weather_dfs)\n",
    "\n",
    "# Group by date and average all numeric weather columns\n",
    "averaged_df = combined_df.groupby(\"time\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Optional: reorder columns\n",
    "cols = [\"time\"] + [col for col in averaged_df.columns if col != \"time\"]\n",
    "averaged_df = averaged_df[cols]\n",
    "\n",
    "# ---------- Save ----------\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "save_path = os.path.join(SAVE_DIR, OUTPUT_FILE)\n",
    "averaged_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ All-India averaged weather data saved to:\\n{save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793627bd-5a0a-4bb9-8377-200fdf1f40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# CONFIG\n",
    "SOURCE_FOLDER = r\"E:\\energy-optimization-project\\manual_forecast_error\"\n",
    "TARGET_ROOT = r\"E:\\energy-optimization-project\\data\\raw\\posoco-forecast-error\"\n",
    "\n",
    "# LOOP\n",
    "for filename in os.listdir(SOURCE_FOLDER):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        try:\n",
    "            # Fix: using %y for 2-digit year\n",
    "            base_name = filename.replace(\".pdf\", \"\")\n",
    "            parsed_date = datetime.strptime(base_name, \"%d-%b-%y\")\n",
    "            folder_name = parsed_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Create target folder\n",
    "            target_folder = os.path.join(TARGET_ROOT, folder_name)\n",
    "            os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "            # Move file\n",
    "            source_file = os.path.join(SOURCE_FOLDER, filename)\n",
    "            target_file = os.path.join(target_folder, filename)\n",
    "            shutil.move(source_file, target_file)\n",
    "\n",
    "            print(f\"‚úÖ Moved: {filename} ‚Üí {folder_name}/\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d3ad0-a2c4-4b8b-be2e-1d314d01406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd==2.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b2c57-ab81-47c2-8364-2e19cd864355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Root folder where your NPP .xls files are stored\n",
    "root_dir = r\"E:\\energy-optimization-project\\data\\raw\\npp\"\n",
    "output_dir = r\"E:\\energy-optimization-project\\data\\processed\\npp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Scan all .xls files inside dated folders\n",
    "xls_files = []\n",
    "for root, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".xls\"):\n",
    "            xls_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"üîç Found {len(xls_files)} XLS files. Starting to process...\")\n",
    "print(\"‚è≥ Take a deep breath, maybe check on Indu ‚Äî I got this!\")\n",
    "\n",
    "# Parse and convert each file\n",
    "for i, file_path in enumerate(tqdm(xls_files, desc=\"üìä Parsing NPP XLS files\"), start=1):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, engine='xlrd')\n",
    "        base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_csv = os.path.join(output_dir, f\"{base_name}.csv\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Done processing all files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1f0f1-a2c3-4fab-92b0-4fd25615f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = r\"E:\\energy-optimization-project\\data\\processed\\npp\"\n",
    "output_file = r\"E:\\energy-optimization-project\\data\\final\\master_npp_data.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "all_data = []\n",
    "skipped_files = []\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty or df.shape[1] == 0:\n",
    "                print(f\"‚ö†Ô∏è Skipped empty file: {file}\")\n",
    "                skipped_files.append(file)\n",
    "                continue\n",
    "            df['source_file'] = file\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {file}: {e}\")\n",
    "            skipped_files.append(file)\n",
    "\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Combined CSV saved at: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nüö´ No valid CSVs found to merge.\")\n",
    "\n",
    "if skipped_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Skipped {len(skipped_files)} files. Check these:\")\n",
    "    for f in skipped_files:\n",
    "        print(\"  -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297d20d-e976-414a-a5f5-94e9991de215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:\\energy-optimization-project\\data\\final\\master_npp_data.csv\")\n",
    "print(\"üî¢ Rows:\", len(df))\n",
    "print(\"üß± Columns:\\n\", df.columns.tolist())\n",
    "print(\"\\nüîç Sample data:\\n\")\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f8e1f-effb-4c1f-bd42-c7f5167051a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path\n",
    "file_path = r\"E:\\energy-optimization-project\\data\\final\\master_npp_data.csv\"\n",
    "save_path = r\"E:\\energy-optimization-project\\data\\final\\cleaned_generation_data.csv\"\n",
    "\n",
    "# Load the raw master file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove fully empty rows\n",
    "df.dropna(how='all', inplace=True)\n",
    "\n",
    "# Fill source_file for grouping\n",
    "df['source_file'].ffill(inplace=True)\n",
    "\n",
    "# Extract only rows that have region names\n",
    "region_keywords = ['All India', 'Northern', 'Western', 'Southern', 'Eastern', 'North Eastern', 'Islands']\n",
    "mask = df['Unnamed: 2'].astype(str).str.strip().isin(region_keywords)\n",
    "\n",
    "# Keep only relevant columns and rows\n",
    "columns_to_keep = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', \n",
    "                   'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'source_file']\n",
    "\n",
    "cleaned_df = df.loc[mask, columns_to_keep].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "cleaned_df.columns = ['Region', 'Installed (MW)', 'Monitored (MW)', 'Annual Target (MU)', 'Gen 21-22 (MU)',\n",
    "                      'Today Program (MU)', 'Today Actual (MU)', 'Apr-1 Program (MU)', 'Apr-1 Actual (MU)', 'Source']\n",
    "\n",
    "# Extract date from source filename\n",
    "cleaned_df['Date'] = cleaned_df['Source'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# Reset index and save\n",
    "cleaned_df.reset_index(drop=True, inplace=True)\n",
    "cleaned_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned generation data saved at:\", save_path)\n",
    "print(\"üî¢ Rows:\", cleaned_df.shape[0])\n",
    "print(\"üß± Columns:\", list(cleaned_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17937ef8-cb12-4e3b-a6cd-aaf45fc52b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"E:\\energy-optimization-project\\data\\final\"\n",
    "output_file = r\"E:\\energy-optimization-project\\data\\final\\cleaned_generation_data.csv\"\n",
    "\n",
    "# Keywords to match regional generation data\n",
    "region_keywords = [\"Northern\", \"Western\", \"Eastern\", \"North Eastern\", \"Southern\", \"All India\"]\n",
    "\n",
    "# Store extracted data\n",
    "all_cleaned = []\n",
    "\n",
    "# Loop through each file\n",
    "file_path = os.path.join(input_dir, \"master_npp_data.csv\")\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "df['source_file'].ffill(inplace=True)\n",
    "\n",
    "# Try to find the right column containing region labels\n",
    "match_cols = [col for col in df.columns if df[col].astype(str).str.contains('|'.join(region_keywords), case=False, na=False).any()]\n",
    "print(f\"üß≠ Detected region names in column(s): {match_cols}\")\n",
    "\n",
    "for col in match_cols:\n",
    "    for _, row in df[df[col].astype(str).str.contains('|'.join(region_keywords), na=False)].iterrows():\n",
    "        values = list(row)\n",
    "        label = row[col]\n",
    "        numbers = [v for v in values if isinstance(v, (int, float)) or (isinstance(v, str) and re.match(r'^\\d+(\\.\\d+)?$', str(v).strip()))]\n",
    "\n",
    "        all_cleaned.append({\n",
    "            \"Region\": label.strip(),\n",
    "            \"Values\": numbers,\n",
    "            \"Source\": row['source_file']\n",
    "        })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "cleaned_df = pd.DataFrame(all_cleaned)\n",
    "\n",
    "# Save\n",
    "cleaned_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Cleaned generation data saved at: {output_file}\")\n",
    "print(f\"üî¢ Rows: {len(cleaned_df)}\")\n",
    "print(f\"üß± Columns: {cleaned_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b9c74-711d-44fb-b119-b20f985995f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfplumber pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72334b1-8ae4-4bc2-bef1-af9fee9d781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# üìÅ Set your input/output paths\n",
    "input_dir = \"E:/energy-optimization-project/data/raw/posoco-psp\"\n",
    "output_file = \"E:/energy-optimization-project/data/final/psp_cleaned_data.csv\"\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(input_dir, file)\n",
    "        try:\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                # Skip the first page (letterhead)\n",
    "                for page in pdf.pages[1:]:\n",
    "                    tables = page.extract_tables()\n",
    "                    for table in tables:\n",
    "                        for row in table:\n",
    "                            if any(cell and str(cell).strip() for cell in row):  # Ignore empty rows\n",
    "                                data_rows.append(row + [file])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {file}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "if data_rows:\n",
    "    max_cols = max(len(row) for row in data_rows)\n",
    "    df = pd.DataFrame(data_rows, columns=[f\"Col{i+1}\" for i in range(max_cols-1)] + [\"source_file\"])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ PSP data saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid data extracted from any files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a0215-b65a-4b73-bc78-2dbaef437b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract pdf2image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d77b5d-c6e9-46d1-b00e-725ec3b87d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Input and output paths\n",
    "input_dir = \"E:/energy-optimization-project/data/raw/posoco-vre\"\n",
    "output_file = \"E:/energy-optimization-project/data/final/vre_extracted_data.csv\"\n",
    "\n",
    "# Function to extract date from filename\n",
    "def extract_date(filename):\n",
    "    match = re.search(r\"\\d{2}\\.\\d{2}\\.\\d{4}\", filename)\n",
    "    return pd.to_datetime(match.group(), format=\"%d.%m.%Y\").date() if match else None\n",
    "\n",
    "# Columns we expect in the table (flexible depending on layout)\n",
    "expected_headers = [\"State\", \"Solar Forecast\", \"Solar Actual\", \"Wind Forecast\", \"Wind Actual\"]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    date = extract_date(file)\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        raw_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            raw_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "        # Attempt to parse the section with state-wise VRE data\n",
    "        pattern = r\"(State.*?)Total.*?\\n\"\n",
    "        matches = re.findall(pattern, raw_text, re.DOTALL)\n",
    "\n",
    "        for match in matches:\n",
    "            lines = match.strip().split(\"\\n\")[1:]  # Skip header row\n",
    "            for line in lines:\n",
    "                parts = re.split(r\"\\s{2,}\", line.strip())\n",
    "                if len(parts) >= 5:\n",
    "                    all_data.append({\n",
    "                        \"Date\": date,\n",
    "                        \"State\": parts[0],\n",
    "                        \"Solar Forecast (MU)\": parts[1],\n",
    "                        \"Solar Actual (MU)\": parts[2],\n",
    "                        \"Wind Forecast (MU)\": parts[3],\n",
    "                        \"Wind Actual (MU)\": parts[4],\n",
    "                        \"Source File\": file\n",
    "                    })\n",
    "\n",
    "        print(f\"‚úÖ Processed: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to process {file}: {e}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ VRE data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31df21-663d-41fb-8467-0e0a912f9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Optional: if tesseract is not in PATH, specify its path\n",
    "# Example for Windows (adjust if installed elsewhere)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Load the saved image\n",
    "image = Image.open(\"psp_page2_preview.png\")\n",
    "\n",
    "# Run OCR\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Save to file for inspection\n",
    "output_path = \"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(extracted_text)\n",
    "\n",
    "print(f\"üìÑ OCR text extracted and saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c1d1f-d6f9-4d78-b6d2-28a3772013dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\"\n",
    "print(\"Exists:\", os.path.exists(file_path))\n",
    "print(\"Readable:\", os.access(file_path,os.R_OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f0dc3-542b-4668-b966-857b9a29d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "ocr_text_path = \"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\"\n",
    "output_csv_path = \"E:/energy-optimization-project/data/final/psp_structured_summary.csv\"\n",
    "\n",
    "# Read OCR text\n",
    "with open(ocr_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract report date\n",
    "date_match = re.search(r\"Date of Reporting: (\\d{2}-\\w{3}-\\d{4})\", text)\n",
    "report_date = date_match.group(1) if date_match else None\n",
    "\n",
    "# Extract Section A block\n",
    "section_a_match = re.search(r\"A\\.\\s*Power Supply Position at All India and Regional level\\s+(.*?)\\nB\\.\", text, re.DOTALL)\n",
    "section_a = section_a_match.group(1).strip() if section_a_match else \"\"\n",
    "\n",
    "# Extract section lines and clean\n",
    "lines = [line.strip() for line in section_a.splitlines() if line.strip()]\n",
    "headers = lines[0].split()\n",
    "records = []\n",
    "\n",
    "for line in lines[1:]:\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 6:\n",
    "        label = \" \".join(parts[:-6])\n",
    "        values = parts[-6:]\n",
    "        records.append([label] + values)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(records, columns=[\"Metric\"] + headers)\n",
    "df[\"Report Date\"] = report_date\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"‚úÖ PSP structured summary saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213db74b-c5bb-4b4d-82ac-a1e1bc33700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output\n",
    "ocr_text_path = \"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\"\n",
    "output_csv_path = \"E:/energy-optimization-project/data/final/psp_sourcewise_generation.csv\"\n",
    "\n",
    "# Read OCR'd text\n",
    "with open(ocr_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract Section G\n",
    "section_g = re.search(r\"G\\.\\s*Sourcewise generation \\(Gross\\) \\(MU\\)(.*?)H\\.\", text, re.DOTALL)\n",
    "if section_g:\n",
    "    lines = [line.strip() for line in section_g.group(1).strip().split(\"\\n\") if line.strip()]\n",
    "    headers = [\"Source\", \"NR\", \"WR\", \"SR\", \"ER\", \"NER\", \"All India\", \"% Share\"]\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = re.split(r'\\s{2,}', line)\n",
    "        if len(parts) >= 8:\n",
    "            data.append(parts[:8])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"‚úÖ Sourcewise generation data saved to: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Section G not found in text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f4e1c-e38e-4ea2-a058-7a8e651afb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "ocr_path = \"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\"\n",
    "output_path = \"E:/energy-optimization-project/data/final/psp_statewise_data.csv\"\n",
    "\n",
    "# Read OCR text\n",
    "with open(ocr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract Section C block\n",
    "section_c = re.search(r\"C\\.\\s*Power Supply Position in States(.*?)D\\.\", text, re.DOTALL)\n",
    "if not section_c:\n",
    "    print(\"‚ö†Ô∏è Section C not found.\")\n",
    "else:\n",
    "    lines = section_c.group(1).split(\"\\n\")\n",
    "    data = []\n",
    "    current_region = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or \"Region\" in line or \"States\" in line:\n",
    "            continue\n",
    "        # Detect region header\n",
    "        region_match = re.match(r\"^(NR|WR|SR|ER|NER)\\s\", line)\n",
    "        if region_match:\n",
    "            current_region = region_match.group(1)\n",
    "            line = line[len(current_region):].strip()\n",
    "\n",
    "        parts = re.split(r'\\s{2,}', line)\n",
    "        if len(parts) >= 7:\n",
    "            state = parts[0]\n",
    "            values = parts[1:8]\n",
    "            data.append([current_region, state] + values)\n",
    "\n",
    "    # Save to CSV\n",
    "    columns = [\"Region\", \"State\", \"Max Demand Met (MW)\", \"Energy Met (MU)\", \n",
    "               \"Drawal Schedule (MU)\", \"OD/UD (MU)\", \"Max OD (MW)\", \"Energy Shortage (MU)\"]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ State-wise data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff4360-6c04-495b-a811-876920260aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def safe_float(val):\n",
    "    return float(re.sub(r\"[^\\d.]+$\", \"\", val.strip().replace(\",\", \".\")))\n",
    "\n",
    "def safe_int(val):\n",
    "    return int(re.sub(r\"\\D+$\", \"\", val.strip()))\n",
    "\n",
    "# Load the raw OCR text\n",
    "with open(\"E:/energy-optimization-project/data/final/psp_ocr_raw_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# --- 1. Extract Statewise Data (Section C) ---\n",
    "state_pattern = re.compile(\n",
    "    r\"(Punjab|Haryana|Rajasthan|Delhi|NR UP\\.?|Uttarakhand|HP\\.?|J&K\\(UT\\) & Ladakh\\(UT\\)|Chandigarh|Chhattisgarh|Guiarat|MP|Maharashtra|Goa|DNHDDPDCL|AMNSIL|BALCO|Andhra Pradesh|Telangana|Karnataka|Kerala|Tamil Nadu|Puducherry|Bihar|DVC|Jharkhand|Odisha|West Bengal|Sikkim|Arunachal Pradesh|Assam|Manipur|Meghalaya|Mizoram|Nagaland|Tripura)\\s+(\\d+)\\s+(\\d+)\\s+([\\d.,]+)\\s+([\\d.,]+)\\s+([-+]?\\d*\\.?\\d+)\\s+(\\d+)\\s+([\\d.,]+)\"\n",
    ")\n",
    "\n",
    "state_rows = []\n",
    "for match in state_pattern.finditer(text):\n",
    "    try:\n",
    "        state, demand, shortage, met, schedule, od_ud, max_od, energy_shortage = match.groups()\n",
    "        state_rows.append({\n",
    "            \"State\": state.replace(\"NR UP.\", \"Uttar Pradesh\"),\n",
    "            \"Max Demand Met (MW)\": safe_int(demand),\n",
    "            \"Shortage (MW)\": safe_int(shortage),\n",
    "            \"Energy Met (MU)\": safe_float(met),\n",
    "            \"Schedule (MU)\": safe_float(schedule),\n",
    "            \"OD/UD (MW)\": safe_float(od_ud),\n",
    "            \"Max OD (MW)\": safe_int(max_od),\n",
    "            \"Energy Shortage (MU)\": safe_float(energy_shortage)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped row due to error: {e}\")\n",
    "\n",
    "# --- 2. Extract Sourcewise Generation (Section G) ---\n",
    "source_pattern = re.compile(\n",
    "    r\"(Coal|Lignite|Hydro|Nuclear|Gas, Naptha & Diesel|RES \\(Wind, Solar, Biomass & Others\\)|Total)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\"\n",
    ")\n",
    "\n",
    "source_rows = []\n",
    "for match in source_pattern.finditer(text):\n",
    "    try:\n",
    "        source, nr, wr, sr, er, ner, all_india, percent_share = match.groups()\n",
    "        source_rows.append({\n",
    "            \"Source\": source,\n",
    "            \"NR\": safe_float(nr),\n",
    "            \"WR\": safe_float(wr),\n",
    "            \"SR\": safe_float(sr),\n",
    "            \"ER\": safe_float(er),\n",
    "            \"NER\": safe_float(ner),\n",
    "            \"All India\": safe_float(all_india),\n",
    "            \"% Share\": int(percent_share)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped row due to error: {e}\")\n",
    "\n",
    "# --- Save CSVs ---\n",
    "state_df = pd.DataFrame(state_rows)\n",
    "source_df = pd.DataFrame(source_rows)\n",
    "\n",
    "state_out = \"E:/energy-optimization-project/data/final/psp_statewise_data.csv\"\n",
    "source_out = \"E:/energy-optimization-project/data/final/psp_sourcewise_generation.csv\"\n",
    "\n",
    "state_df.to_csv(state_out, index=False)\n",
    "source_df.to_csv(source_out, index=False)\n",
    "\n",
    "print(f\"‚úÖ Statewise data saved to: {state_out}\")\n",
    "print(f\"‚úÖ Sourcewise data saved to: {source_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d36ce-03b8-4a18-be42-84c8706db4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_dir = \"E:/energy-optimization-project/data/raw/posoco-psp\"\n",
    "statewise_output = \"E:/energy-optimization-project/data/final/psp_statewise_data.csv\"\n",
    "sourcewise_output = \"E:/energy-optimization-project/data/final/psp_sourcewise_generation.csv\"\n",
    "log_file = \"E:/energy-optimization-project/data/final/processed_psp_files.txt\"\n",
    "\n",
    "# --- SET PATHS ---\n",
    "poppler_path = r\"C:\\poppler\\poppler-24.07.0\\Library\\bin\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# --- LOAD PROCESSED FILES ---\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, \"r\") as f:\n",
    "        processed_files = set(f.read().splitlines())\n",
    "else:\n",
    "    processed_files = set()\n",
    "\n",
    "# --- LOAD EXISTING DATA ---\n",
    "statewise_data = pd.read_csv(statewise_output) if os.path.exists(statewise_output) else pd.DataFrame()\n",
    "sourcewise_data = pd.read_csv(sourcewise_output) if os.path.exists(sourcewise_output) else pd.DataFrame()\n",
    "\n",
    "# --- REGEX PATTERNS ---\n",
    "state_pattern = re.compile(\n",
    "    r\"([A-Za-z &().]+)\\s+(\\d+)\\s+(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([-‚Äì.\\d]+)\\s+(\\d+)\\s+([\\d.]+)\"\n",
    ")\n",
    "\n",
    "source_pattern = re.compile(\n",
    "    r\"(Coal|Lignite|Hydro|Nuclear|Gas, Naptha & Diesel|RES \\(Wind, Solar, Biomass & Others\\)|Total)\"\n",
    "    r\"\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\"\n",
    ")\n",
    "\n",
    "# --- PROCESS ---\n",
    "all_state_rows = []\n",
    "all_source_rows = []\n",
    "total_files = sum(len(files) for _, _, files in os.walk(input_dir))\n",
    "done_count = 0\n",
    "\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file_path in processed_files:\n",
    "                done_count += 1\n",
    "                print(f\"[{done_count}/{total_files}] ‚è≠Ô∏è Skipping: {file}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract Date\n",
    "                date_match = re.search(r\"(\\d{2})[.-](\\d{2})[.-](\\d{4})\", file)\n",
    "                date_str = f\"{date_match.group(3)}-{date_match.group(2)}-{date_match.group(1)}\" if date_match else \"unknown\"\n",
    "\n",
    "                # OCR from page 2\n",
    "                images = convert_from_path(file_path, dpi=300, first_page=2, last_page=2, poppler_path=poppler_path)\n",
    "                text = pytesseract.image_to_string(images[0])\n",
    "\n",
    "                # --- Statewise ---\n",
    "                for match in state_pattern.finditer(text):\n",
    "                    try:\n",
    "                        state, demand, shortage, met, schedule, od_ud, max_od, energy_shortage = match.groups()\n",
    "                        state = state.replace(\"NR UP.\", \"Uttar Pradesh\")\n",
    "                        all_state_rows.append({\n",
    "                            \"Date\": date_str,\n",
    "                            \"State\": state,\n",
    "                            \"Max Demand Met (MW)\": int(demand),\n",
    "                            \"Shortage (MW)\": int(shortage),\n",
    "                            \"Energy Met (MU)\": float(met),\n",
    "                            \"Schedule (MU)\": float(schedule),\n",
    "                            \"OD/UD (MW)\": float(od_ud),\n",
    "                            \"Max OD (MW)\": int(max_od),\n",
    "                            \"Energy Shortage (MU)\": float(energy_shortage)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Row error in {file}: {e}\")\n",
    "\n",
    "                # --- Sourcewise ---\n",
    "                for match in source_pattern.finditer(text):\n",
    "                    try:\n",
    "                        src, nr, wr, sr, er, ner, all_india, share = match.groups()\n",
    "                        all_source_rows.append({\n",
    "                            \"Date\": date_str,\n",
    "                            \"Source\": src,\n",
    "                            \"NR\": float(nr),\n",
    "                            \"WR\": float(wr),\n",
    "                            \"SR\": float(sr),\n",
    "                            \"ER\": float(er),\n",
    "                            \"NER\": float(ner),\n",
    "                            \"All India\": float(all_india),\n",
    "                            \"% Share\": int(share)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Source error in {file}: {e}\")\n",
    "\n",
    "                # Save processed log\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(file_path + \"\\n\")\n",
    "\n",
    "                done_count += 1\n",
    "                print(f\"[{done_count}/{total_files}] ‚úÖ Processed: {file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{done_count + 1}/{total_files}] ‚ùå Failed: {file} ‚Äî {e}\")\n",
    "\n",
    "# --- APPEND + SAVE ---\n",
    "if all_state_rows:\n",
    "    df_new_state = pd.DataFrame(all_state_rows)\n",
    "    statewise_data = pd.concat([statewise_data, df_new_state], ignore_index=True)\n",
    "    statewise_data.to_csv(statewise_output, index=False)\n",
    "\n",
    "if all_source_rows:\n",
    "    df_new_source = pd.DataFrame(all_source_rows)\n",
    "    sourcewise_data = pd.concat([sourcewise_data, df_new_source], ignore_index=True)\n",
    "    sourcewise_data.to_csv(sourcewise_output, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ All data saved and script finished!\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711d658-6533-4435-9d0b-341c811c713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all cleaned files\n",
    "gen_df = pd.read_csv(\"E:/energy-optimization-project/data/final/cleaned_generation_data.csv\")\n",
    "state_df = pd.read_csv(\"E:/energy-optimization-project/data/final/psp_statewise_data.csv\")\n",
    "source_df = pd.read_csv(\"E:/energy-optimization-project/data/final/psp_sourcewise_generation.csv\")\n",
    "vre_df = pd.read_csv(\"E:/energy-optimization-project/data/final/vre_extracted_data.csv\")\n",
    "\n",
    "# Ensure date format consistency\n",
    "gen_df['Date'] = pd.to_datetime(gen_df['Date'])\n",
    "state_df['Date'] = pd.to_datetime(state_df['Date'])\n",
    "source_df['Date'] = pd.to_datetime(source_df['Date'])\n",
    "vre_df['Date'] = pd.to_datetime(vre_df['Date'])\n",
    "\n",
    "# Merge generation + sourcewise generation\n",
    "merged = pd.merge(gen_df, source_df, on=\"Date\", how=\"outer\")\n",
    "\n",
    "# Merge with VRE data\n",
    "merged = pd.merge(merged, vre_df, on=\"Date\", how=\"outer\")\n",
    "\n",
    "# Save this intermediate step\n",
    "merged.to_csv(\"E:/energy-optimization-project/data/final/master_generation_combined.csv\", index=False)\n",
    "\n",
    "# Optional: merge state-level (if needed for regional/state dashboards)\n",
    "# state-level will be handled separately in most visualizations\n",
    "state_df.to_csv(\"E:/energy-optimization-project/data/final/master_statewise_data.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Merged master dataset saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5f322-5830-48db-8664-1488822add13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1dc06-97cb-4045-b5fb-1bfadd04b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "input_root = \"E:/energy-optimization-project/data/raw/posoco-vre\"\n",
    "output_folder = \"E:/energy-optimization-project/data/final\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "summary_csv = os.path.join(output_folder, \"vre_daily_summary.csv\")\n",
    "remc_csv = os.path.join(output_folder, \"vre_remc_profile.csv\")\n",
    "curtailment_csv = os.path.join(output_folder, \"vre_curtailment_data.csv\")\n",
    "\n",
    "summary_rows, remc_rows, curtailment_rows = [], [], []\n",
    "\n",
    "def extract_from_text(text, date):\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # --- 1. Daily Summary ---\n",
    "    summary_match = re.search(\n",
    "        r\"Date of reporting:\\s*(\\d{1,2}-\\w+-\\d{4})\\s+Report for\\s*:\\s*(\\d{1,2}-\\w+-\\d{4}).*?\"\n",
    "        r\"Maximum Demand met \\(MW\\)\\s*([\\d,]+).*?Maximum Demand met time.*?(\\d{1,2}:\\d{2})\"\n",
    "        r\".*?Wind energy Contribution \\(MW\\)\\s*([\\d,]+).*?Solar energy Contribution.*?([\\d,]+)\"\n",
    "        r\".*?Variable Renewable.*?\\(MW\\)\\s*([\\d,]+)\", text, re.DOTALL\n",
    "    )\n",
    "    if summary_match:\n",
    "        summary_rows.append({\n",
    "            \"Report Date\": summary_match.group(2),\n",
    "            \"Max Demand Met (MW)\": int(summary_match.group(3).replace(\",\", \"\")),\n",
    "            \"Max Demand Time\": summary_match.group(4),\n",
    "            \"Wind Contribution (MW)\": int(summary_match.group(5).replace(\",\", \"\")),\n",
    "            \"Solar Contribution (MW)\": int(summary_match.group(6).replace(\",\", \"\")),\n",
    "            \"Total VRE Contribution (MW)\": int(summary_match.group(7).replace(\",\", \"\")),\n",
    "        })\n",
    "\n",
    "    # --- 2. REMC Profile ---\n",
    "    remc_pattern = re.compile(\n",
    "        r\"(?:‡§™‡§µ‡§®|Wind)\\s*([\\d,]+)\\s*([\\d,]+)\\s*([\\d,]+).*?(\\d{1,2}:\\d{2}).*?(\\d{1,2}:\\d{2}).*?\"\n",
    "        r\"([\\d.]+)\\s*([\\d.]+)\\s*([-\\d.]+)\\s*([\\d.]+)\", re.DOTALL\n",
    "    )\n",
    "    for match in remc_pattern.finditer(text):\n",
    "        remc_rows.append({\n",
    "            \"Date\": date,\n",
    "            \"Schedule (MW)\": int(match.group(1).replace(\",\", \"\")),\n",
    "            \"Actual (MW)\": int(match.group(2).replace(\",\", \"\")),\n",
    "            \"Deviation (MW)\": int(match.group(3).replace(\",\", \"\")),\n",
    "            \"Max Gen Time\": match.group(4),\n",
    "            \"Min Gen Time\": match.group(5),\n",
    "            \"Schedule (MU)\": float(match.group(6)),\n",
    "            \"Actual (MU)\": float(match.group(7)),\n",
    "            \"Deviation (MU)\": float(match.group(8)),\n",
    "            \"CUF (%)\": float(match.group(9)),\n",
    "        })\n",
    "\n",
    "    # --- 3. Curtailment ---\n",
    "    curtailment_pattern = re.compile(\n",
    "        r\"(?:(\\w+(?:\\s+\\w+)*?)\\s*/\\s*RE State.*?)?([-\\d.]+)\\s+MU.*?([\\d,]+)\\s*MW\", re.DOTALL\n",
    "    )\n",
    "    for match in curtailment_pattern.finditer(text):\n",
    "        name = match.group(1)\n",
    "        if name:\n",
    "            curtailment_rows.append({\n",
    "                \"Date\": date,\n",
    "                \"Region/State\": name.strip(),\n",
    "                \"Curtailment (MU)\": float(match.group(2)),\n",
    "                \"Max Curtailment (MW)\": int(match.group(3).replace(\",\", \"\")),\n",
    "            })\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "folders = sorted(os.listdir(input_root))\n",
    "processed = 0\n",
    "\n",
    "print(f\"üìÇ Found {len(folders)} folders to scan\")\n",
    "for folder in tqdm(folders, desc=\"üìÑ Extracting from VRE PDFs\"):\n",
    "    folder_path = os.path.join(input_root, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        folder_date_obj = datetime.strptime(folder, \"%Y-%m-%d\")\n",
    "        date_str_dot = folder_date_obj.strftime(\"%d.%m.%Y\")\n",
    "    except ValueError:\n",
    "        print(f\"‚ö†Ô∏è Skipping invalid folder name: {folder}\")\n",
    "        continue\n",
    "\n",
    "    # PDF name format example: 01.01.2025_NLDC_REMC_REPORT.pdf\n",
    "    pdf_name = f\"{date_str_dot}_NLDC_REMC_REPORT.pdf\"\n",
    "    pdf_path = os.path.join(folder_path, pdf_name)\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"‚ö†Ô∏è PDF not found: {pdf_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\\n\".join([page.get_text() for page in doc])\n",
    "        extract_from_text(full_text, folder)\n",
    "        processed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {pdf_name}: {e}\")\n",
    "\n",
    "# --- Save outputs ---\n",
    "if summary_rows:\n",
    "    pd.DataFrame(summary_rows).to_csv(summary_csv, index=False)\n",
    "if remc_rows:\n",
    "    pd.DataFrame(remc_rows).to_csv(remc_csv, index=False)\n",
    "if curtailment_rows:\n",
    "    pd.DataFrame(curtailment_rows).to_csv(curtailment_csv, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! {processed} PDFs processed and data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68171fc-0e4c-4816-891b-a671f863ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === PATH SETUP ===\n",
    "input_root = \"E:/energy-optimization-project/data/raw/posoco-vre\"\n",
    "output_dir = \"E:/energy-optimization-project/data/final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Output files\n",
    "daily_summary_csv = os.path.join(output_dir, \"vre_daily_summary.csv\")\n",
    "remc_csv = os.path.join(output_dir, \"vre_remc_profile.csv\")\n",
    "curtailment_csv = os.path.join(output_dir, \"vre_curtailment_data.csv\")\n",
    "\n",
    "# === HELPERS ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def parse_daily_summary(text):\n",
    "    summary_pattern = re.compile(\n",
    "        r\"Solar hrs.?\\n(.+?)\\n.?Non Solar hrs.?\\n(.+?)\\n.?Maximum Wind.*?\\n(.+?)\\n\",\n",
    "        re.DOTALL\n",
    "    )\n",
    "    match = summary_pattern.search(text)\n",
    "    if not match:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    solar_line = match.group(1).split()\n",
    "    non_solar_line = match.group(2).split()\n",
    "    max_line = match.group(3).split()\n",
    "\n",
    "    summary = [\n",
    "        [\"Solar Hrs\"] + solar_line[:5],\n",
    "        [\"Non-Solar Hrs\"] + non_solar_line[:5],\n",
    "        [\"Max Values\"] + max_line\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(summary, columns=[\"Type\", \"Demand Met (MW)\", \"Time\", \"Wind (MW)\", \"Solar (MW)\", \"VRE (MW)\"])\n",
    "    return df\n",
    "\n",
    "def parse_remc_profile(text):\n",
    "    table_blocks = re.findall(r\"‡§™‡§µ‡§®\\s*/\\s*Wind.*?(?=\\n\\n|\\Z)\", text, re.DOTALL)\n",
    "    data = []\n",
    "    for block in table_blocks:\n",
    "        if \"‡§∏‡•å‡§∞ / Solar\" in block:\n",
    "            values = re.findall(r\"[-]?\\d+\\.\\d+|[-]?\\d+\", block)\n",
    "            if len(values) >= 13:\n",
    "                data.append(values[:13])\n",
    "    columns = [\n",
    "        \"Schedule MW\", \"Actual MW\", \"Deviation MW\", \"Time1\", \"Time2\",\n",
    "        \"MU_Schedule\", \"MU_Actual\", \"MU_Deviation\", \"CUF %\",\n",
    "        \"Extra_1\", \"Extra_2\", \"Extra_3\", \"Extra_4\"\n",
    "    ]\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "def parse_curtailment_data(text):\n",
    "    curtail_section = re.search(r\"8\\..?Curtailment.?Details.*?\\n(.+?)(?=9\\.)\", text, re.DOTALL)\n",
    "    if not curtail_section:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    block = curtail_section.group(1)\n",
    "    lines = [line.strip() for line in block.split(\"\\n\") if line.strip() and not line.startswith(\"Note\")]\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if \"Nil\" in line or \"--\" in line:\n",
    "            continue\n",
    "        values = re.findall(r\"[A-Za-z/()&.\\-]+|\\d+\\.\\d+|\\d+\", line)\n",
    "        if len(values) >= 4:\n",
    "            data.append(values[:4])\n",
    "    return pd.DataFrame(data, columns=[\"Region/State\", \"Source\", \"Curtailment (MU)\", \"Curtailment (MW)\"])\n",
    "\n",
    "# === MAIN SCRIPT ===\n",
    "all_summary = []\n",
    "all_remc = []\n",
    "all_curtailment = []\n",
    "\n",
    "folders = sorted([f for f in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, f))])\n",
    "print(f\"üìÇ Found {len(folders)} folders to scan\")\n",
    "\n",
    "for folder in tqdm(folders, desc=\"üìÑ Extracting from VRE PDFs\"):\n",
    "    folder_path = os.path.join(input_root, folder)\n",
    "    try:\n",
    "        day, month, year = folder[-2:], folder[-5:-3], folder[:4]\n",
    "        pdf_name = f\"{day}.{month}.{year}_NLDC_REMC_REPORT.pdf\"\n",
    "        pdf_path = os.path.join(folder_path, pdf_name)\n",
    "\n",
    "        if not os.path.isfile(pdf_path):\n",
    "            continue\n",
    "\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        summary = parse_daily_summary(text)\n",
    "        remc = parse_remc_profile(text)\n",
    "        curtailment = parse_curtailment_data(text)\n",
    "\n",
    "        if not summary.empty:\n",
    "            summary.insert(0, \"Date\", folder)\n",
    "            all_summary.append(summary)\n",
    "\n",
    "        if not remc.empty:\n",
    "            remc.insert(0, \"Date\", folder)\n",
    "            all_remc.append(remc)\n",
    "\n",
    "        if not curtailment.empty:\n",
    "            curtailment.insert(0, \"Date\", folder)\n",
    "            all_curtailment.append(curtailment)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in folder {folder}: {e}\")\n",
    "\n",
    "# Save all collected data\n",
    "if all_summary:\n",
    "    pd.concat(all_summary).to_csv(daily_summary_csv, index=False)\n",
    "\n",
    "if all_remc:\n",
    "    pd.concat(all_remc).to_csv(remc_csv, index=False)\n",
    "\n",
    "if all_curtailment:\n",
    "    pd.concat(all_curtailment).to_csv(curtailment_csv, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ All available PDFs processed and data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d68c8-b39c-4fcd-b5ef-7814920917a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def parse_daily_summary(text):\n",
    "    try:\n",
    "        summary_pattern = re.compile(\n",
    "            r\"Solar hrs.*?\\n([\\d,]+)\\s+(\\d{1,2}:\\d{2})\\s+([\\d,]+)\\s+([\\d,]+)\\s+([\\d,]+).*?\"\n",
    "            r\"Non Solar hrs.*?\\n([\\d,]+)\\s+(\\d{1,2}:\\d{2})\\s+([\\d,]+)\\s+([\\d,]+)\\s+([\\d,]+).*?\"\n",
    "            r\"Maximum Wind.*?\\n([\\d,]+)\\s+(\\d{1,2}:\\d{2}).*?\"\n",
    "            r\"Maximum Solar.*?\\n([\\d,]+)\\s+(\\d{1,2}:\\d{2}).*?\"\n",
    "            r\"Maximum Variable.*?\\n([\\d,]+)\\s+(\\d{1,2}:\\d{2}).*?\"\n",
    "            r\"Maximum Wind penetration.*?([\\d.]+)%\\s+(\\d{1,2}:\\d{2}).*?\"\n",
    "            r\"Maximum Solar penetration.*?([\\d.]+)%\\s+(\\d{1,2}:\\d{2}).*?\"\n",
    "            r\"Maximum VRE penetration.*?([\\d.]+)%\\s+(\\d{1,2}:\\d{2})\",\n",
    "            re.DOTALL\n",
    "        )\n",
    "        match = summary_pattern.search(text)\n",
    "        if not match:\n",
    "            return None\n",
    "\n",
    "        fields = match.groups()\n",
    "        return pd.DataFrame([{\n",
    "            \"Solar Hrs Demand (MW)\": fields[0],\n",
    "            \"Solar Hrs Time\": fields[1],\n",
    "            \"Solar Hrs Wind (MW)\": fields[2],\n",
    "            \"Solar Hrs Solar (MW)\": fields[3],\n",
    "            \"Solar Hrs VRE (MW)\": fields[4],\n",
    "            \"Non-Solar Hrs Demand (MW)\": fields[5],\n",
    "            \"Non-Solar Hrs Time\": fields[6],\n",
    "            \"Non-Solar Hrs Wind (MW)\": fields[7],\n",
    "            \"Non-Solar Hrs Solar (MW)\": fields[8],\n",
    "            \"Non-Solar Hrs VRE (MW)\": fields[9],\n",
    "            \"Max Wind Gen (MW)\": fields[10],\n",
    "            \"Max Wind Gen Time\": fields[11],\n",
    "            \"Max Solar Gen (MW)\": fields[12],\n",
    "            \"Max Solar Gen Time\": fields[13],\n",
    "            \"Max VRE Gen (MW)\": fields[14],\n",
    "            \"Max VRE Gen Time\": fields[15],\n",
    "            \"Max Wind Penetration (%)\": fields[16],\n",
    "            \"Max Wind Pen Time\": fields[17],\n",
    "            \"Max Solar Penetration (%)\": fields[18],\n",
    "            \"Max Solar Pen Time\": fields[19],\n",
    "            \"Max VRE Penetration (%)\": fields[20],\n",
    "            \"Max VRE Pen Time\": fields[21],\n",
    "        }])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_curtailment_data(text):\n",
    "    try:\n",
    "        section = re.search(r\"8\\..*?Curtailment.*?Details.*?\\n(.+?)(?=9\\.)\", text, re.DOTALL)\n",
    "        if not section:\n",
    "            return None\n",
    "        block = section.group(1)\n",
    "        lines = [line for line in block.split(\"\\n\") if line.strip() and \"Nil\" not in line and \"--\" not in line]\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            values = re.findall(r\"([A-Za-z&/()\\-\\s]+)\\s+([\\d.]+)\\s+MU.*?([\\d,]+)\\s*MW\", line)\n",
    "            if values:\n",
    "                name, mu, mw = values[0]\n",
    "                data.append({\n",
    "                    \"Region/State\": name.strip(),\n",
    "                    \"Curtailment (MU)\": float(mu),\n",
    "                    \"Curtailment (MW)\": int(mw.replace(\",\", \"\"))\n",
    "                })\n",
    "        return pd.DataFrame(data) if data else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def main(root_dir):\n",
    "    output_dir = os.path.join(\"E:/energy-optimization-project/data/final\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    summary_all, curtailment_all = [], []\n",
    "\n",
    "    folders = sorted([\n",
    "        f for f in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, f))\n",
    "    ])\n",
    "\n",
    "    print(f\"üìÇ Found {len(folders)} folders to scan\")\n",
    "\n",
    "    for folder in tqdm(folders, desc=\"üìÑ Extracting from VRE PDFs\"):\n",
    "        date_str = folder  # e.g. 2025-01-01\n",
    "        pdf_name = f\"{folder[8:10]}.{folder[5:7]}.{folder[0:4]}_NLDC_REMC_REPORT.pdf\"\n",
    "        pdf_path = os.path.join(root_dir, folder, pdf_name)\n",
    "\n",
    "        if not os.path.exists(pdf_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            daily_summary_df = parse_daily_summary(text)\n",
    "            if daily_summary_df is not None:\n",
    "                daily_summary_df.insert(0, \"Date\", date_str)\n",
    "                summary_all.append(daily_summary_df)\n",
    "\n",
    "            curtail_df = parse_curtailment_data(text)\n",
    "            if curtail_df is not None:\n",
    "                curtail_df.insert(0, \"Date\", date_str)\n",
    "                curtailment_all.append(curtail_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in {pdf_path}: {e}\")\n",
    "\n",
    "    # Save CSVs\n",
    "    if summary_all:\n",
    "        pd.concat(summary_all).to_csv(os.path.join(output_dir, \"vre_daily_summary.csv\"), index=False)\n",
    "    if curtailment_all:\n",
    "        pd.concat(curtailment_all).to_csv(os.path.join(output_dir, \"vre_curtailment_data.csv\"), index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ All available PDFs processed and data saved.\")\n",
    "\n",
    "# Run with POSOCO VRE folder\n",
    "main(\"E:/energy-optimization-project/data/raw/posoco-vre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576bd45a-a314-4761-ac14-7469a6f21af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Paths ===\n",
    "final_dir = \"E:/energy-optimization-project/data/final\"\n",
    "output_file = os.path.join(final_dir, \"merged_master_dataset.csv\")\n",
    "\n",
    "# === Load datasets ===\n",
    "npp_df = pd.read_csv(os.path.join(final_dir, \"master_npp_data.csv\"))\n",
    "psp_state_df = pd.read_csv(os.path.join(final_dir, \"psp_statewise_data.csv\"))\n",
    "psp_source_df = pd.read_csv(os.path.join(final_dir, \"psp_sourcewise_generation.csv\"))\n",
    "vre_summary_df = pd.read_csv(os.path.join(final_dir, \"vre_daily_summary.csv\"))\n",
    "remc_df = pd.read_csv(os.path.join(final_dir, \"vre_remc_profile.csv\"))\n",
    "\n",
    "# === Function to standardize 'Date' column ===\n",
    "def standardize_date(df, date_col):\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True).dt.date\n",
    "    df.rename(columns={date_col: \"Date\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "# === Standardize date columns ===\n",
    "npp_df = standardize_date(npp_df, \"Date\")\n",
    "psp_state_df = standardize_date(psp_state_df, \"Date\")\n",
    "psp_source_df = standardize_date(psp_source_df, \"Date\")\n",
    "vre_summary_df = standardize_date(vre_summary_df, \"Report Date\")\n",
    "remc_df = standardize_date(remc_df, \"Date\")\n",
    "\n",
    "# === Merge datasets one by one ===\n",
    "merged = npp_df.copy()\n",
    "\n",
    "# Merge with VRE summary (one-to-one)\n",
    "merged = pd.merge(merged, vre_summary_df, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Merge PSP Source-wise\n",
    "merged = pd.merge(merged, psp_source_df, on=\"Date\", how=\"left\", suffixes=('', '_psp_source'))\n",
    "\n",
    "# Merge PSP State-wise\n",
    "merged = pd.merge(merged, psp_state_df, on=\"Date\", how=\"left\", suffixes=('', '_psp_state'))\n",
    "\n",
    "# Aggregate REMC data per day (in case of multiple rows per day)\n",
    "remc_agg = remc_df.groupby(\"Date\").agg({\n",
    "    \"Schedule (MW)\": \"sum\",\n",
    "    \"Actual (MW)\": \"sum\",\n",
    "    \"Deviation (MW)\": \"sum\",\n",
    "    \"Schedule (MU)\": \"sum\",\n",
    "    \"Actual (MU)\": \"sum\",\n",
    "    \"Deviation (MU)\": \"sum\",\n",
    "    \"CUF (%)\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "merged = pd.merge(merged, remc_agg, on=\"Date\", how=\"left\", suffixes=('', '_remc'))\n",
    "\n",
    "# === Save merged output ===\n",
    "merged.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Merged dataset saved to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ecfb5-0819-4b94-9a6a-a5894825a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Setup ===\n",
    "file_path = \"E:/energy-optimization-project/data/final/master_npp_data.csv\"\n",
    "\n",
    "# Rename map\n",
    "col_map = {\n",
    "    \"Unnamed: 0\": \"REGION\",\n",
    "    \"Unnamed: 1\": \"STATE\",\n",
    "    \"Unnamed: 2\": \"SECTOR\",\n",
    "    \"Unnamed: 3\": \"TYPE\",\n",
    "    \"Unnamed: 4\": \"STATION NAME\",\n",
    "    \"Unnamed: 5\": \"UNIT\",\n",
    "    \"Unnamed: 6\": \"TODAY'S GENERATION (MU)\",\n",
    "    \"Unnamed: 7\": \"TODAY'S PROG. GENERATION (MU)\",\n",
    "    \"Unnamed: 8\": \"APRIL 1 TILL DATE - PROG. (MU)\",\n",
    "    \"Unnamed: 9\": \"APRIL 1 TILL DATE - ACTUAL (MU)\",\n",
    "    \"Unnamed: 10\": \"COAL\",\n",
    "    \"Unnamed: 11\": \"CAPACITY OUTAGE (MW)\",\n",
    "    \"Unnamed: 14\": \"SOURCE FILE\"\n",
    "}\n",
    "\n",
    "# Read only needed columns\n",
    "use_cols = list(col_map.keys())\n",
    "df = pd.read_csv(file_path, usecols=use_cols, low_memory=False)\n",
    "df.rename(columns=col_map, inplace=True)\n",
    "\n",
    "# Remove rows without SOURCE FILE\n",
    "df = df[df[\"SOURCE FILE\"].notna()].copy()\n",
    "\n",
    "# Extract date with tqdm\n",
    "tqdm.pandas(desc=\"Extracting date\")\n",
    "df[\"Date\"] = df[\"SOURCE FILE\"].progress_apply(lambda x: re.search(r\"\\d{4}-\\d{2}-\\d{2}\", x).group() if re.search(r\"\\d{4}-\\d{2}-\\d{2}\", x) else None)\n",
    "df.dropna(subset=[\"Date\"], inplace=True)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"cleaned_master_npp_data.csv\", index=False)\n",
    "print(\"‚úÖ Done: saved to cleaned_master_npp_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fd776-403f-47a0-a6a0-0489c3cbd4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === Set directory ===\n",
    "final_dir = \"E:/energy-optimization-project/data/final\"\n",
    "\n",
    "# === Load all cleaned DGR files ===\n",
    "dgr1 = pd.read_csv(os.path.join(final_dir, \"cleaned_dgr1.csv\"))\n",
    "dgr1[\"section\"] = \"dgr1\"\n",
    "\n",
    "dgr2 = pd.read_csv(os.path.join(final_dir, \"cleaned_dgr2.csv\"))\n",
    "dgr2[\"section\"] = \"dgr2\"\n",
    "\n",
    "dgr16 = pd.read_csv(os.path.join(final_dir, \"cleaned_dgr16.csv\"))\n",
    "dgr16[\"section\"] = \"dgr16\"\n",
    "\n",
    "dgr17 = pd.read_csv(os.path.join(final_dir, \"cleaned_dgr17.csv\"))\n",
    "dgr17[\"section\"] = \"dgr17\"\n",
    "\n",
    "# === Combine all ===\n",
    "combined_df = pd.concat([dgr1, dgr2, dgr16, dgr17], ignore_index=True)\n",
    "\n",
    "# === Optional: Standardize Date column ===\n",
    "combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# === Save final output ===\n",
    "output_path = os.path.join(final_dir, \"final_combined_dgr_dataset.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Final combined DGR dataset saved at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9019fd6-b8fe-4fe4-906d-4c6b1a7324c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the CSV file from the specified path\n",
    "df = pd.read_csv('E:/energy-optimization-project/data/final/psp_sourcewise_generation.csv')\n",
    "\n",
    "# Ensure 'Source' column is treated as string\n",
    "df['Source'] = df['Source'].astype(str)\n",
    "\n",
    "# Identify the indices of rows starting with 'Coal' and ending with 'Total'\n",
    "coal_indices = df.index[df['Source'].str.lower() == 'coal'].tolist()\n",
    "total_indices = df.index[df['Source'].str.lower() == 'total'].tolist()\n",
    "\n",
    "# Build non-overlapping blocks of 'Coal' to next 'Total'\n",
    "blocks = []\n",
    "for i in tqdm(range(len(coal_indices)), desc='Processing Sourcewise Blocks'):\n",
    "    start = coal_indices[i]\n",
    "    # Find the first 'Total' after the current 'Coal'\n",
    "    next_totals = [idx for idx in total_indices if idx > start]\n",
    "    if not next_totals:\n",
    "        continue\n",
    "    end = next_totals[0]\n",
    "    blocks.append((start, end))\n",
    "\n",
    "# Assign dates starting from 2022-01-01\n",
    "start_date = pd.to_datetime(\"2022-01-01\")\n",
    "date_series = pd.Series(index=df.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "for i, (start, end) in enumerate(tqdm(blocks, desc=\"Assigning Dates\")):\n",
    "    current_date = start_date + pd.Timedelta(days=i)\n",
    "    date_series[start:end + 1] = current_date\n",
    "\n",
    "# Add the date column to the original DataFrame\n",
    "df['Date'] = date_series\n",
    "\n",
    "# Save the updated DataFrame to the same directory\n",
    "output_path = 'E:/energy-optimization-project/data/final/psp_sourcewise_generation_dated.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b9a7c-68bb-435f-a0ed-6b09113fa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "final_dir = \"E:/energy-optimization-project/data/final\"\n",
    "\n",
    "# Load datasets\n",
    "dgr = pd.read_csv(os.path.join(final_dir, \"final_combined_dgr_dataset.csv\"))\n",
    "psp_source = pd.read_csv(os.path.join(final_dir, \"psp_sourcewise_generation_dated.csv\"))\n",
    "psp_state = pd.read_csv(os.path.join(final_dir, \"psp_statewise_data_dated.csv\"))\n",
    "vre_summary = pd.read_csv(os.path.join(final_dir, \"vre_daily_summary.csv\"))\n",
    "vre_remc_path = os.path.join(final_dir, \"vre_remc_profile.csv\")\n",
    "vre_remc = pd.read_csv(vre_remc_path) if os.path.exists(vre_remc_path) else pd.DataFrame()\n",
    "\n",
    "# Standardize 'Date' column to datetime\n",
    "def standardize(df):\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date\n",
    "    return df\n",
    "\n",
    "dgr = standardize(dgr)\n",
    "psp_source = standardize(psp_source)\n",
    "psp_state = standardize(psp_state)\n",
    "vre_summary = standardize(vre_summary)\n",
    "vre_remc = standardize(vre_remc)\n",
    "\n",
    "# Merge all datasets on 'Date' (outer to preserve all entries)\n",
    "merged_df = psp_source.merge(psp_state, on=\"Date\", how=\"outer\", suffixes=(\"_source\", \"_state\"))\n",
    "merged_df = merged_df.merge(vre_summary, on=\"Date\", how=\"outer\")\n",
    "if not vre_remc.empty:\n",
    "    merged_df = merged_df.merge(vre_remc, on=\"Date\", how=\"outer\")\n",
    "\n",
    "# Optionally merge DGR if you want full data (can be skipped for performance)\n",
    "# merged_df = merged_df.merge(dgr, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Save the final merged output\n",
    "output_path = os.path.join(final_dir, \"phase1_fused_energy_data.csv\")\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Merged dataset saved at:\\n{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f5de0d-cc5a-46f3-ad3d-2da51ab553b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined weather dataset saved at:\n",
      "E:/energy-optimization-project/data/final/open_meteo_combined_weather.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the folder with all city-wise weather CSVs\n",
    "weather_dir = \"E:/energy-optimization-project/data/raw/weather\"\n",
    "\n",
    "# Create an empty list to collect dataframes\n",
    "weather_data = []\n",
    "\n",
    "# Loop over all CSVs in the directory\n",
    "for file in os.listdir(weather_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        city = file.replace(\".csv\", \"\").title()\n",
    "        df = pd.read_csv(os.path.join(weather_dir, file))\n",
    "        df[\"city\"] = city\n",
    "        weather_data.append(df)\n",
    "\n",
    "# Concatenate all into a single DataFrame\n",
    "combined_weather = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "# Save to final directory\n",
    "final_path = \"E:/energy-optimization-project/data/final/open_meteo_combined_weather.csv\"\n",
    "combined_weather.to_csv(final_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Combined weather dataset saved at:\\n{final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6798c01b-1a67-4141-b934-37d814ca3a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23224\\3257602554.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  weights /= weights.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hourly output saved to:\n",
      "E:\\energy-optimization-project\\data\\final\\hourly_vre_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = r\"E:\\energy-optimization-project\\data\\final\\merged_vre_weather.csv\"\n",
    "output_path = r\"E:\\energy-optimization-project\\data\\final\\hourly_vre_output.csv\"\n",
    "\n",
    "SOLAR_EFFICIENCY = 0.18  # 18%\n",
    "WIND_CUT_IN = 3\n",
    "WIND_RATED = 12\n",
    "WIND_CUT_OUT = 25\n",
    "WIND_CAPACITY_MW = 2\n",
    "RADIATION_CONVERSION = 0.2778  # MJ/m¬≤ to kWh/m¬≤\n",
    "\n",
    "city_coordinates = {\n",
    "    \"Ahmedabad\": (23.0225, 72.5714),\n",
    "    \"Delhi\": (28.6139, 77.2090),\n",
    "    \"Guwahati\": (26.1445, 91.7362),\n",
    "    \"Rewa\": (24.5300, 81.3000),\n",
    "    \"Bengaluru\": (12.9716, 77.5946),\n",
    "    \"Kolkata\": (22.5726, 88.3639),\n",
    "}\n",
    "\n",
    "# === LOAD DATA ===\n",
    "df = pd.read_csv(input_path, parse_dates=[\"Date\"])\n",
    "df = df[df[\"city\"].isin(city_coordinates)]\n",
    "\n",
    "hourly_records = []\n",
    "\n",
    "# === WIND POWER CURVE ===\n",
    "def wind_power_output(speed):\n",
    "    if speed < WIND_CUT_IN or speed >= WIND_CUT_OUT:\n",
    "        return 0\n",
    "    elif speed < WIND_RATED:\n",
    "        return WIND_CAPACITY_MW * ((speed - WIND_CUT_IN) / (WIND_RATED - WIND_CUT_IN)) ** 3\n",
    "    else:\n",
    "        return WIND_CAPACITY_MW\n",
    "\n",
    "# === MAIN PROCESSING LOOP ===\n",
    "for _, row in df.iterrows():\n",
    "    city = row[\"city\"]\n",
    "    date = pd.to_datetime(row[\"Date\"]).date()\n",
    "    lat, lon = city_coordinates[city]\n",
    "    sw_rad = row[\"shortwave_radiation_sum\"]\n",
    "    wind_speed = row[\"wind_speed_10m_max\"]\n",
    "\n",
    "    location = LocationInfo(city, \"India\", \"Asia/Kolkata\", lat, lon)\n",
    "    s = sun(location.observer, date=date)\n",
    "    sunrise = s[\"sunrise\"].replace(tzinfo=None)\n",
    "    sunset = s[\"sunset\"].replace(tzinfo=None)\n",
    "\n",
    "    start_hour = sunrise.hour\n",
    "    end_hour = sunset.hour\n",
    "    daylight_hours = max(1, end_hour - start_hour)\n",
    "\n",
    "    # Convert MJ/m¬≤ ‚Üí kWh/m¬≤ ‚Üí MWh\n",
    "    radiation_kwh = sw_rad * RADIATION_CONVERSION\n",
    "    daily_solar_mwh = radiation_kwh * SOLAR_EFFICIENCY\n",
    "\n",
    "    # Bell curve solar profile\n",
    "    solar_profile = np.zeros(24)\n",
    "    hours = np.arange(24)\n",
    "    mid = (start_hour + end_hour) / 2\n",
    "    spread = daylight_hours / 5\n",
    "    weights = np.exp(-0.5 * ((hours - mid) / spread) ** 2)\n",
    "    weights[:start_hour] = 0\n",
    "    weights[end_hour+1:] = 0\n",
    "    weights /= weights.sum()\n",
    "    solar_profile = weights * daily_solar_mwh\n",
    "\n",
    "    # Wind profile\n",
    "    np.random.seed(int(date.strftime('%j')))\n",
    "    fluctuations = np.clip(np.random.normal(1.0, 0.1, 24), 0.7, 1.3)\n",
    "    wind_speeds = wind_speed * fluctuations\n",
    "    wind_profile = [wind_power_output(ws) for ws in wind_speeds]\n",
    "\n",
    "    for hour in range(24):\n",
    "        hourly_records.append({\n",
    "            \"Date\": date,\n",
    "            \"Hour\": hour,\n",
    "            \"City\": city,\n",
    "            \"Solar_MW\": round(solar_profile[hour], 3),\n",
    "            \"Wind_MW\": round(wind_profile[hour], 3),\n",
    "        })\n",
    "\n",
    "# === SAVE OUTPUT ===\n",
    "hourly_df = pd.DataFrame(hourly_records)\n",
    "hourly_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Hourly output saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b741f4c-c803-4f6e-82d8-6acb1bc1e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting astral\n",
      "  Downloading astral-3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astral) (2023.3)\n",
      "Downloading astral-3.2-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: astral\n",
      "Successfully installed astral-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install astral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edaa63e-5e61-467e-893c-9e6cfc3ec5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
